{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b =  [0.11948588 0.05694906 0.26137693 0.04176397 0.07714815 0.0220334\n",
      " 0.12616168 0.06698716 0.05799734 0.12461754 0.04547888]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis)\n",
    "\n",
    "\n",
    "def tensor_dot(q, k):\n",
    "    b = softmax((k @ q) / np.sqrt(q.shape[0]))\n",
    "    return b\n",
    "\n",
    "\n",
    "i_query = np.random.normal(size=(4,))\n",
    "i_keys = np.random.normal(size=(11, 4))\n",
    "\n",
    "b = tensor_dot(i_query, i_keys)\n",
    "print(\"b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.70352711, -0.30749631])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention_layer(q, k, v):\n",
    "    b = tensor_dot(q, k)\n",
    "    return b @ v\n",
    "\n",
    "\n",
    "i_values = np.random.normal(size=(11, 2))\n",
    "attention_layer(i_query, i_keys, i_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14166611,  0.82768644, -0.20830393, -0.13968515],\n",
       "       [ 0.62958833,  1.52217987, -0.98164199, -0.81337288],\n",
       "       [-0.37721688,  0.51519563,  1.01461165, -0.50778403],\n",
       "       [-0.15620989,  0.52947983, -0.00580699, -0.22095754],\n",
       "       [-0.49284163,  0.70659222,  1.29013892, -0.81564934],\n",
       "       [-0.21432032,  1.20800275, -0.44441405, -0.27398989],\n",
       "       [-0.3590753 ,  0.64706809,  0.71811405, -0.49860559],\n",
       "       [-1.32671927,  0.48657072,  1.42693456, -0.13747287],\n",
       "       [-0.26645251,  0.71149397,  0.23760909,  0.06369302],\n",
       "       [-1.70293759,  0.64473755,  0.7384292 ,  0.53364533],\n",
       "       [-0.26003857,  0.63070486, -0.30909207, -0.01952356]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_tensor_dot(q, k):\n",
    "    # a will be batch x seq x feature dim\n",
    "    # which is N x N x 4\n",
    "    # batched dot product in einstein notation\n",
    "    a = np.einsum(\"ij,kj->ik\", q, k) / np.sqrt(q.shape[0])\n",
    "    # now we softmax over sequence\n",
    "    b = softmax(a, axis=1)\n",
    "    return b\n",
    "\n",
    "\n",
    "def self_attention(x):\n",
    "    b = batched_tensor_dot(x, x)\n",
    "    return b @ x\n",
    "\n",
    "\n",
    "i_batched_query = np.random.normal(size=(11, 4))\n",
    "self_attention(i_batched_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12769163, 0.09124732, 0.03532783, 0.09123936, 0.03330473,\n",
       "        0.12311383, 0.05293832, 0.02072694, 0.10300264, 0.03439216,\n",
       "        0.11929049],\n",
       "       [0.15402048, 0.51885061, 0.03003771, 0.13019326, 0.0341949 ,\n",
       "        0.16123542, 0.05393516, 0.00488009, 0.05912303, 0.00521977,\n",
       "        0.10893934],\n",
       "       [0.04492095, 0.0226277 , 0.20560824, 0.07523942, 0.16833385,\n",
       "        0.02918269, 0.13766222, 0.10261309, 0.08268766, 0.03375067,\n",
       "        0.02883877]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_tensor_dot(i_batched_query, i_batched_query)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.30191444e+00,  1.33519393e+01],\n",
       "       [-1.07976527e+01,  1.71696788e+01],\n",
       "       [-2.06228584e-01,  5.26121744e-01],\n",
       "       [-2.07197807e+00,  4.16864276e+00],\n",
       "       [-7.24454804e+00,  1.73452878e+01],\n",
       "       [-3.09215914e+02,  5.69863172e+02],\n",
       "       [-7.20045416e+00,  1.60051619e+01],\n",
       "       [-1.29332446e+00,  3.26045502e+00],\n",
       "       [-8.53170329e-01,  8.68704900e-01],\n",
       "       [-5.67902393e+00,  1.03121008e+01],\n",
       "       [-3.09908251e+00,  4.59573106e+00]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights should be input feature_dim -> desired output feature_dim\n",
    "w_q = np.random.normal(size=(4, 4))\n",
    "w_k = np.random.normal(size=(4, 4))\n",
    "w_v = np.random.normal(size=(4, 2))\n",
    "\n",
    "\n",
    "def trainable_self_attention(x, w_q, w_k, w_v):\n",
    "    q = x @ w_q\n",
    "    k = x @ w_k\n",
    "    v = x @ w_v\n",
    "    b = batched_tensor_dot(q, k)\n",
    "    return b @ v\n",
    "\n",
    "\n",
    "trainable_self_attention(i_batched_query, w_q, w_k, w_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.41295587e-01, -1.24346344e-02],\n",
       "       [ 5.22764962e+00, -7.50054109e+00],\n",
       "       [-1.73216017e-01,  2.90437943e-01],\n",
       "       [ 1.14987061e-01, -2.44625446e-01],\n",
       "       [ 2.81643642e+00, -1.71539036e+00],\n",
       "       [ 3.54703195e+00,  7.77289153e-02],\n",
       "       [ 8.32457972e-01, -3.56496920e-01],\n",
       "       [ 1.24207454e+00,  4.60469372e+00],\n",
       "       [ 2.77992592e-01,  5.84268211e-01],\n",
       "       [ 2.96992346e+02,  3.59071906e+02],\n",
       "       [ 1.24000150e+00,  1.35369927e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q_h1 = np.random.normal(size=(4, 4))\n",
    "w_k_h1 = np.random.normal(size=(4, 4))\n",
    "w_v_h1 = np.random.normal(size=(4, 2))\n",
    "w_q_h2 = np.random.normal(size=(4, 4))\n",
    "w_k_h2 = np.random.normal(size=(4, 4))\n",
    "w_v_h2 = np.random.normal(size=(4, 2))\n",
    "w_h = np.random.normal(size=2)\n",
    "\n",
    "\n",
    "def multihead_attention(x, w_q_h1, w_k_h1, w_v_h1, w_q_h2, w_k_h2, w_v_h2):\n",
    "    h1_out = trainable_self_attention(x, w_q_h1, w_k_h1, w_v_h1)\n",
    "    h2_out = trainable_self_attention(x, w_q_h2, w_k_h2, w_v_h2)\n",
    "    # join along last axis so we can use dot.\n",
    "    all_h = np.stack((h1_out, h2_out), -1)\n",
    "    return all_h @ w_h\n",
    "\n",
    "\n",
    "multihead_attention(i_batched_query, w_q_h1, w_k_h1, w_v_h1, w_q_h2, w_k_h2, w_v_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
